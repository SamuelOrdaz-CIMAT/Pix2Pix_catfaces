# Pix2Pix: GeneraciÃ³n de Rostros de Gatos desde Landmarks

ImplementaciÃ³n de **Pix2Pix** (Conditional GAN) para generar imÃ¡genes realistas de rostros de gatos a partir de mapas de calor de landmarks faciales. El proyecto utiliza el dataset **CatFLW** y arquitecturas U-Net + PatchGAN para lograr una traducciÃ³n imagen-a-imagen de alta calidad.

## ğŸ¯ Objetivo

Transformar representaciones abstractas de landmarks faciales (9 puntos clave: ojos, nariz, boca, orejas) en imÃ¡genes fotorrealistas de gatos mediante aprendizaje adversarial condicional.

## ğŸ—ï¸ Arquitectura

### Generador: U-Net
- **Input**: 1 canal (heatmap) â†’ **Output**: 3 canales (RGB)
- **Encoder**: 8 capas de downsampling (64 â†’ 512 canales)
- **Bottleneck**: Capa de 512 canales en resoluciÃ³n 1Ã—1
- **Decoder**: 7 capas de upsampling con skip connections U-Net
- **NormalizaciÃ³n**: InstanceNorm2d (mejor estabilidad que BatchNorm)
- **Dropout**: 50% en las primeras 3 capas del decoder
- **Salida**: Tanh activation para normalizaciÃ³n [-1, 1]

### Discriminador: PatchGAN
- **Input**: 4 canales (heatmap 1ch + imagen RGB 3ch concatenados)
- Clasificador de parches 30Ã—30 para evaluar detalles locales
- 5 capas convolucionales (64 â†’ 512 canales)
- **NormalizaciÃ³n**: InstanceNorm2d (excepto primera capa)
- Salida sin activaciÃ³n (usa MSELoss directamente)

## ğŸ“Š Dataset: CatFLW

- **Total**: 2,079 imÃ¡genes de rostros de gatos
- **Split**: 1,663 train / 207 val / 209 test (~80%/10%/10%)
- **ResoluciÃ³n**: 256Ã—256 pÃ­xeles
- **Landmarks**: 9 puntos faciales clave (ojos, nariz, boca, orejas)
- **Formato**:
  - **A (Input)**: Mapas de calor Gaussianos de 1 canal (.npy, uint8)
  - **B (Target)**: ImÃ¡genes RGB (.jpg, calidad 95)
- **Preprocesamiento**: 
  - Adaptive bounding box con expansiÃ³n dinÃ¡mica
  - Mapas de calor Gaussianos (Ïƒ configurable, default=2.0)
  - NormalizaciÃ³n [-1, 1] para ambos A y B
- **Augmentation** (solo training):
  - Flip horizontal sincronizado entre A y B
  - ColorJitter ligero (brightness, contrast, saturation, hue)

## ğŸ”§ ConfiguraciÃ³n de Entrenamiento

### ParÃ¡metros Actuales
```python
epochs = 100
batch_size = 24
lambda_L1 = 150         # PÃ©rdida de reconstrucciÃ³n L1
lambda_perc = 2         # PÃ©rdida perceptual VGG16
lr = 0.0002             # Learning rate Generator
lr_D = 0.0001           # Learning rate Discriminator (50% de lr_G)
betas = (0.5, 0.999)
```

### Funciones de PÃ©rdida
1. **Adversarial Loss**: MSELoss (LSGAN) sin label smoothing
   - Real labels: 1.0
   - Fake labels: 0.0
   - MÃ¡s estable que BCEWithLogitsLoss para este caso
2. **L1 Loss**: ReconstrucciÃ³n pixel-a-pixel (Î»=150)
3. **Perceptual Loss**: Features VGG16 (layers [:16], Î»=2)

### TÃ©cnicas de OptimizaciÃ³n
- **Scheduler**: CosineAnnealingWarmRestarts (T_0=50, T_mult=2)
- **Discriminador**: Entrenado en cada batch (no cada N batches)
- **SelecciÃ³n de modelo**: Solo por Val L1 (PSNR solo informativo)
- **Checkpointing**: 
  - `last_checkpoint.pth`: Guarda estado completo cada Ã©poca
  - `G_best.pth` / `D_best.pth`: Guarda cuando mejora Val L1
- **Resume automÃ¡tico**: Si existe checkpoint, continÃºa desde ahÃ­
- **Data Augmentation** (solo training):
  - HorizontalFlip sincronizado (50%)
  - ColorJitter muy ligero (brightness=0.05, contrast=0.05, saturation=0.05, hue=0.01)

## ğŸ“ˆ Resultados (Ã‰poca ~70)

| MÃ©trica | Valor | Objetivo |
|---------|-------|----------|
| **Loss D** | ~0.47 | 0.3-0.7 âœ… |
| **Loss G** | ~30 | 20-40 âœ… |
| **Val L1** | ~0.49 | <0.20 âš ï¸ |
| **PSNR** | ~4-5 dB | >20 dB âš ï¸ |

### Estado Actual
- âœ… **Equilibrio adversarial estable** (D y G convergen sin colapso)
- âœ… **Estructura facial correcta** (landmarks â†’ posiciÃ³n facial precisa)
- âœ… **InstanceNorm**: Mejor estabilidad que BatchNorm
- âœ… **Checkpoint resume**: Funcional y probado
- âš ï¸ **Blur moderado**: Detalles finos suavizados (bigotes, textura de pelaje)

### Observaciones
- `lambda_L1=150` alto â†’ prioriza reconstrucciÃ³n promedio sobre detalles
- PSNR usado solo como mÃ©trica descriptiva, no influye en entrenamiento
- Val L1 es la Ãºnica mÃ©trica para selecciÃ³n del mejor modelo

## ğŸš€ Uso

### InstalaciÃ³n
```bash
pip install -r requirements.txt
```

O manualmente:
```bash
pip install torch torchvision opencv-python numpy pillow tqdm matplotlib
```

### Preparar Dataset
Usando el script `generate_dataset.py`:
```bash
python generate_dataset.py \
    --input_root "CatFLW dataset" \
    --output_root "datasets/catflw" \
    --sigma 2.0 \
    --seed 42
```

O desde Python:
```python
from generate_dataset import prepare_catflw_dataset

prepare_catflw_dataset(
    input_root="CatFLW dataset",
    output_root="datasets/catflw",
    sigma=2.0,
    min_coverage=0.9
)
```

### Entrenar Modelo
```python
# Entrenamiento desde cero
G = train_pix2pix(
    dataset_root="datasets/catflw",
    epochs=100,
    batch_size=24,
    lambda_L1=150,
    lambda_perc=2,
    lr=0.0002,
    resume=False  # False para empezar desde cero
)

# Reanudar desde checkpoint (automÃ¡tico)
G = train_pix2pix(
    dataset_root="datasets/catflw",
    epochs=100,
    batch_size=24,
    lambda_L1=150,
    lambda_perc=2,
    lr=0.0002,
    resume=True  # True por defecto
)
```

### Evaluar en Test
```python
# Evaluar con mejor modelo guardado
avg_l1, avg_psnr = evaluate_on_test(G, dataset_root="datasets/catflw/test")

# Mostrar mejores ejemplos visuales
test_loader = DataLoader(
    Pix2PixDataset("datasets/catflw/test"), 
    batch_size=24,
    shuffle=False
)
show_samples(G, test_loader, device, n=16, 
            title="Mejores Resultados", 
            save_path="results/best_samples_test.png")
```

### Reanudar Entrenamiento
**AutomÃ¡tico**: Si `resume=True` (default) y existe `checkpoints/last_checkpoint.pth`, el entrenamiento continÃºa desde la Ãºltima Ã©poca guardada, preservando:
- Estado de G y D
- Estado de optimizadores (momentum, etc.)
- Mejor Val L1 registrado
- NÃºmero de Ã©poca

## ğŸ“ Estructura del Proyecto

```
pix2pix/
â”œâ”€â”€ pix2pix_proyect.ipynb      # Notebook principal con todo el pipeline
â”œâ”€â”€ generate_dataset.py         # Script de preprocesamiento CatFLW â†’ Pix2Pix
â”œâ”€â”€ README.md                   # Este archivo
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ .gitignore                  # ConfiguraciÃ³n de exclusiones
â”œâ”€â”€ CatFLW dataset/            # Dataset original (NO versionado)
â”‚   â”œâ”€â”€ images/                # 2,079 imÃ¡genes .png originales
â”‚   â””â”€â”€ labels/                # 2,079 archivos .json con landmarks
â”œâ”€â”€ datasets/                   # Dataset procesado (NO versionado)
â”‚   â””â”€â”€ catflw/
â”‚       â”œâ”€â”€ train/             # 1,663 pares
â”‚       â”‚   â”œâ”€â”€ A/             # Heatmaps .npy (1 canal)
â”‚       â”‚   â””â”€â”€ B/             # ImÃ¡genes .jpg (RGB)
â”‚       â”œâ”€â”€ val/               # 207 pares
â”‚       â”‚   â”œâ”€â”€ A/
â”‚       â”‚   â””â”€â”€ B/
â”‚       â””â”€â”€ test/              # 209 pares
â”‚           â”œâ”€â”€ A/
â”‚           â””â”€â”€ B/
â”œâ”€â”€ checkpoints/               # Modelos guardados
â”‚   â”œâ”€â”€ G_best.pth            # Mejor generador por Val L1
â”‚   â”œâ”€â”€ D_best.pth            # Mejor discriminador
â”‚   â””â”€â”€ last_checkpoint.pth   # Ãšltimo estado completo (NO versionado)
â””â”€â”€ results/                   # Visualizaciones generadas
    â”œâ”€â”€ dataset_samples.png    # Ejemplos del dataset
    â”œâ”€â”€ epoch_{N}.png          # Progreso cada 10 Ã©pocas (NO versionadas)
    â””â”€â”€ best_samples_test.png  # Mejores resultados en test
```

## ğŸ”„ PrÃ³ximos Pasos

### OptimizaciÃ³n para Mejorar Detalles
1. **Reducir lambda_L1**: 150 â†’ 100 â†’ 60 (menos over-smoothing)
2. **Reducir lambda_perc**: 2 â†’ 1 (menos dependencia de VGG)
3. **Sigma mÃ¡s definido**: Regenerar dataset con Ïƒ=1.5 (landmarks mÃ¡s sharp)
4. **Continuar entrenamiento**: 100 â†’ 200 Ã©pocas
5. **Evaluar arquitectura**: Considerar attention mechanisms

### Expectativas con Ajustes
- **Val L1**: 0.15-0.25
- **PSNR**: 15-25 dB
- **Calidad visual**: Mejor definiciÃ³n en bigotes, pelaje y ojos

## ğŸ“ Historial de Versiones

### v2.0 - Noviembre 2025 (Actual)
- âœ… Implementado checkpoint resume automÃ¡tico
- âœ… Simplificado pipeline: sin early stopping, sin PSNR en entrenamiento
- âœ… InstanceNorm en lugar de BatchNorm para mejor estabilidad
- âœ… SelecciÃ³n de mejor modelo solo por Val L1
- âœ… Script `generate_dataset.py` con adaptive bounding box
- âœ… README actualizado con documentaciÃ³n completa
- ğŸ”„ Entrenamiento en progreso (~70 Ã©pocas completadas)

### v1.0 - Baseline
- Entrenamiento inicial hasta Ã©poca 200
- Lambda ajustado en Ã©poca 101 (100/5 â†’ 60/3)
- Identificado problema de blur por lambda_L1 alto

## ğŸ› ï¸ TecnologÃ­as

- **PyTorch 2.x**: Framework de deep learning
- **CUDA**: AceleraciÃ³n GPU (si disponible)
- **torchvision**: Transformaciones y VGG16 preentrenado para perceptual loss
- **OpenCV**: Procesamiento de imÃ¡genes en preprocesamiento
- **NumPy**: Operaciones de array y carga de heatmaps .npy
- **Pillow (PIL)**: Carga y manipulaciÃ³n de imÃ¡genes
- **tqdm**: Barras de progreso
- **matplotlib**: VisualizaciÃ³n de resultados

## ğŸ‘¨â€ğŸ’» Autor

**Samuel Ordaz**  
ğŸ“§ samuel.ordaz@cimat.mx

## ğŸ“„ Licencia

Este proyecto es parte de investigaciÃ³n acadÃ©mica en Deep Learning.

## ğŸ™ Referencias

- Isola, P., et al. (2017). "Image-to-Image Translation with Conditional Adversarial Networks" (Pix2Pix paper)
- Ronneberger, O., et al. (2015). "U-Net: Convolutional Networks for Biomedical Image Segmentation"
- Dataset CatFLW: Facial Landmarks in the Wild for Cats
