# üé® Gu√≠a Completa: Pix2Pix para Generaci√≥n de Im√°genes desde Landmarks

## üìö Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Conceptos Fundamentales](#conceptos-fundamentales)
3. [Arquitectura del Modelo](#arquitectura-del-modelo)
4. [Preparaci√≥n del Dataset](#preparaci√≥n-del-dataset)
5. [Entrenamiento](#entrenamiento)
6. [Evaluaci√≥n y M√©tricas](#evaluaci√≥n-y-m√©tricas)
7. [Flujo Completo del C√≥digo](#flujo-completo-del-c√≥digo)

---

## üéØ Introducci√≥n

### ¬øQu√© es Pix2Pix?

**Pix2Pix** es una arquitectura de **GAN condicional (cGAN)** que aprende a traducir im√°genes de un dominio a otro:
- **Input**: Imagen condicional (landmarks/m√°scara)
- **Output**: Imagen realista generada

**Aplicaciones**:
- Boceto ‚Üí Foto realista
- Mapa ‚Üí Imagen satelital
- D√≠a ‚Üí Noche
- **Nuestro caso**: Landmarks faciales ‚Üí Cara de gato

---

## üß† Conceptos Fundamentales

### 1. **Redes Generativas Adversarias (GANs)**

Dos redes que compiten entre s√≠:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Generador  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇDiscriminador ‚îÇ
‚îÇ     (G)     ‚îÇ  Fake   ‚îÇ     (D)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚Üë                        ‚îÇ
      ‚îÇ                        ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Feedback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **Generador (G)**: Crea im√°genes falsas intentando enga√±ar al discriminador
- **Discriminador (D)**: Clasifica si una imagen es real o falsa

**Objetivo**: G mejora generando im√°genes m√°s realistas, D mejora detectando falsas.

---

### 2. **Conditional GAN (cGAN)**

A diferencia de GANs cl√°sicas, **Pix2Pix es condicional**:

```
Input (Landmarks) + Ruido ‚Üí Generador ‚Üí Output (Cara)
                              ‚Üì
            Discriminador compara: Input + Output vs Input + Real
```

**Ventaja**: Control total sobre la salida (condicionada al input).

---

### 3. **U-Net Generator**

Arquitectura **encoder-decoder** con **skip connections**:

```
Input (256√ó256)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Encoder (Downsampling)             ‚îÇ
‚îÇ  64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 512 (√ó3)    ‚îÇ  Extrae caracter√≠sticas
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì Bottleneck (512)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Decoder (Upsampling)               ‚îÇ
‚îÇ  512 ‚Üí 512 (√ó3) ‚Üí 256 ‚Üí 128 ‚Üí 64    ‚îÇ  Reconstruye imagen
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output (256√ó256√ó3)
```

**Skip connections** (concatenaciones):
- Conectan capas del encoder directamente al decoder
- Preservan detalles finos (bordes, texturas)
- Evitan p√©rdida de informaci√≥n espacial

#### C√≥digo U-Net:

```python
# Encoder
d1 = self.down1(x)        # 256‚Üí128 (64 channels)
d2 = self.down2(d1)       # 128‚Üí64  (128 channels)
# ... m√°s capas

# Decoder con skip connections
u1 = self.up1(bottleneck)
u2 = self.up2(torch.cat([u1, d7], 1))  # ‚Üê Skip connection
```

---

### 4. **PatchGAN Discriminator**

No clasifica la imagen completa, sino **parches peque√±os** (30√ó30):

```
Imagen 256√ó256 ‚Üí Convs ‚Üí Salida 30√ó30√ó1
                          ‚Üì
                Cada p√≠xel = probabilidad de ese parche ser real
```

**Ventajas**:
- Menos par√°metros que un discriminador global
- Enfoca en detalles locales (texturas)
- Mejor para im√°genes de alta resoluci√≥n

#### Arquitectura:

```python
Conv(4, 64)  ‚Üí LeakyReLU
Conv(64, 128) + BN ‚Üí LeakyReLU
Conv(128, 256) + BN ‚Üí LeakyReLU
Conv(256, 512) + BN ‚Üí LeakyReLU
Conv(512, 1)  # Sin activaci√≥n (usa BCEWithLogitsLoss)
```

---

## üì¶ Preparaci√≥n del Dataset

### 1. **Mapas Gaussianos para Landmarks**

En lugar de puntos discretos, creamos **heatmaps suaves**:

```python
def create_heatmap_landmarks(coords_scaled, size=256, sigma=4):
    heatmap = np.zeros((size, size))
    coeff = 1.0 / (2 * sigma**2)
    
    for x, y in coords_scaled:
        # Gaussiana 2D centrada en (x, y)
        dist_sq = (x_grid - x)**2 + (y_grid - y)**2
        heatmap += np.exp(-dist_sq * coeff)
```

**¬øPor qu√© Gaussianas?**
- M√°s informaci√≥n espacial que puntos binarios
- El generador aprende mejor con gradientes suaves
- Sigma bajo (3-4) = landmarks m√°s definidos

**Visualizaci√≥n**:
```
Punto (x, y)   ‚Üí   Gaussiana œÉ=4
     *         ‚Üí      .:*.:.
                      :*#*:
                      .:*:.
```

---

### 2. **Estructura del Dataset**

```
datasets/catflw/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ A/  ‚Üê Mapas Gaussianos (landmarks)
‚îÇ   ‚îî‚îÄ‚îÄ B/  ‚Üê Im√°genes reales
‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ A/
‚îÇ   ‚îî‚îÄ‚îÄ B/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ A/
    ‚îî‚îÄ‚îÄ B/
```

**Split**: 80% train, 10% val, 10% test

---

### 3. **Transformaciones (Data Augmentation)**

```python
# Para A (landmarks)
transforms.Resize((256, 256), interpolation=NEAREST)  # Preserva valores discretos
transforms.RandomHorizontalFlip(p=0.5)
transforms.ToTensor()
transforms.Normalize([0.5], [0.5])  # [-1, 1]

# Para B (im√°genes)
transforms.Resize((256, 256), interpolation=BILINEAR)  # Suaviza
transforms.RandomHorizontalFlip(p=0.5)
transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)  # Variaci√≥n de color
transforms.ToTensor()
transforms.Normalize([0.5]*3, [0.5]*3)  # [-1, 1]
```

---

## üéì Entrenamiento

### 1. **Funciones de P√©rdida**

#### a) **GAN Loss (Adversarial)**

```python
criterion_GAN = nn.BCEWithLogitsLoss()

# Para Discriminador:
loss_D = 0.5 * (loss_real + loss_fake)

# Para Generador:
loss_GAN = criterion_GAN(pred_fake, ones)  # Quiere enga√±ar a D
```

**Concepto**: G intenta maximizar la probabilidad de que D clasifique sus salidas como reales.

---

#### b) **L1 Loss (Reconstrucci√≥n)**

```python
criterion_L1 = nn.L1Loss()
loss_L1 = criterion_L1(fake_B, B) * lambda_L1  # Œª=100
```

**¬øPor qu√© L1 y no L2 (MSE)?**
- **L1** penaliza diferencias absolutas ‚Üí menos blur
- **L2** penaliza diferencias al cuadrado ‚Üí im√°genes m√°s borrosas
- L1 preserva mejor bordes y detalles

**Analog√≠a**:
```
Error = 10 p√≠xeles
L1: penaliza 10
L2: penaliza 100 (10¬≤) ‚Üí sobreenaltiza errores grandes ‚Üí promedia colores ‚Üí blur
```

---

#### c) **Perceptual Loss (VGG)**

```python
vgg = models.vgg16(...).features[:16]  # Capas convolucionales

def perceptual_loss(fake, real):
    return F.l1_loss(vgg(fake), vgg(real))  # Compara features
```

**Concepto**: En lugar de comparar p√≠xel a p√≠xel, compara **caracter√≠sticas sem√°nticas** extra√≠das por VGG.

**Ventajas**:
- Mejor percepci√≥n humana de similitud
- Preserva texturas y estructuras de alto nivel

---

#### d) **P√©rdida Total del Generador**

```python
loss_G = loss_GAN + loss_L1 * 100 + loss_perceptual * 10
         ‚Üë           ‚Üë                ‚Üë
     Enga√±ar D   Parecerse       Texturas/sem√°ntica
                 p√≠xel a p√≠xel
```

**Pesos t√≠picos**:
- `lambda_L1 = 100`: Fuerte penalizaci√≥n por diferencias p√≠xel
- `lambda_perc = 10`: Moderado enfoque en features

---

### 2. **Label Smoothing con Ruido**

```python
# En lugar de usar 1.0 y 0.0:
real_label = 0.9 + 0.1 * torch.rand_like(pred_real)  # [0.9, 1.0]
fake_label = 0.1 * torch.rand_like(pred_fake)        # [0.0, 0.1]
```

**Beneficios**:
- Previene overconfidence del discriminador
- Estabiliza entrenamiento GAN
- Reduce mode collapse

---

### 3. **Automatic Mixed Precision (AMP)**

```python
scaler = torch.cuda.amp.GradScaler()

with torch.amp.autocast("cuda"):
    fake_B = G(A)
    loss_G = ...

scaler.scale(loss_G).backward()
scaler.step(opt_G)
scaler.update()
```

**¬øQu√© hace?**
- Usa **FP16** (16-bit) para forward/backward ‚Üí **2√ó m√°s r√°pido**
- Mantiene **FP32** (32-bit) para actualizaciones de pesos ‚Üí estabilidad
- `GradScaler` escala gradientes para evitar underflow

**Ventaja**: Entrenar el doble de r√°pido con misma GPU.

---

### 4. **Learning Rate Scheduling**

```python
scheduler = LambdaLR(opt, lr_lambda=lambda e: 1 - max(0, e - epochs/2) / (epochs/2))
```

**Comportamiento**:
```
Epochs 1-100:    lr = 2e-4 (constante)
Epochs 101-200:  lr decae linealmente ‚Üí 0
```

**Raz√≥n**: Al inicio necesita explorar, al final afinar detalles.

---

### 5. **Checkpoints y Reanudaci√≥n**

```python
torch.save({
    "epoch": epoch,
    "G_state": G.state_dict(),
    "D_state": D.state_dict(),
    "opt_G_state": opt_G.state_dict(),
    "opt_D_state": opt_D.state_dict(),
    "sched_G_state": scheduler_G.state_dict(),
    "sched_D_state": scheduler_D.state_dict(),
    "scaler_state": scaler.state_dict(),
    "best_val_loss": best_val_loss
}, "checkpoints/last_checkpoint.pth")
```

**Tipos de checkpoint**:
- `last_checkpoint.pth`: √öltima √©poca (para reanudar)
- `G_best.pth`, `D_best.pth`: Mejor modelo (menor val loss)
- `G_epoch{N}.pth`: Snapshots cada 50 √©pocas

---

## üìä Evaluaci√≥n y M√©tricas

### 1. **PSNR (Peak Signal-to-Noise Ratio)**

```python
def psnr(pred, target):
    mse = torch.mean((pred - target) ** 2)
    return 20 * torch.log10(1.0 / torch.sqrt(mse))
```

**Interpretaci√≥n**:
- Mide similitud p√≠xel a p√≠xel
- **Mayor = mejor** (t√≠picamente 20-40 dB)
- PSNR > 30 dB ‚Üí buena calidad

---

### 2. **SSIM (Structural Similarity Index)**

```python
import piq
ssim_value = piq.ssim(pred, target, data_range=2.0)
```

**Concepto**: Mide similitud estructural (luminancia, contraste, estructura) en lugar de p√≠xeles brutos.

**Interpretaci√≥n**:
- Rango: [0, 1]
- **1.0 = id√©nticas**
- SSIM > 0.9 ‚Üí muy buena calidad
- Correlaciona mejor con percepci√≥n humana que PSNR

---

### 3. **L1 Loss**

```python
criterion_L1(fake_B, B)  # Diferencia promedio absoluta
```

**Interpretaci√≥n**:
- Menor = m√°s parecido
- T√≠picamente 0.05-0.20 en im√°genes normalizadas

---

## üîÑ Flujo Completo del C√≥digo

### **Paso 1: Preparar Dataset**

```python
prepare_catflw_dataset("CatFLW dataset", "datasets/catflw", sigma=3)
```

1. Lee im√°genes y JSONs con landmarks
2. Recorta caras usando bounding boxes
3. Escala a 256√ó256
4. Genera heatmaps Gaussianos para landmarks
5. Guarda pares A (landmarks) y B (im√°genes)

---

### **Paso 2: Crear DataLoaders**

```python
train_dataset = Pix2PixDataset(root, augment=True)
train_loader = DataLoader(train_dataset, batch_size=32, ...)
```

- Carga pares (A, B)
- Aplica augmentation (flips, color jitter)
- Normaliza a [-1, 1]

---

### **Paso 3: Inicializar Modelos**

```python
G = UNetGenerator(in_ch=1, out_ch=3)  # 1 canal ‚Üí 3 canales RGB
D = PatchDiscriminator(in_ch=4)       # 1 (A) + 3 (B) = 4 canales
```

---

### **Paso 4: Loop de Entrenamiento**

```python
for epoch in range(epochs):
    for A, B in train_loader:
        # 1) Entrenar Discriminador
        fake_B = G(A).detach()
        loss_D = BCE(D(A, B), real) + BCE(D(A, fake_B), fake)
        
        # 2) Entrenar Generador
        fake_B = G(A)
        loss_G = BCE(D(A, fake_B), real) + L1(fake_B, B) + Perc(fake_B, B)
        
    # 3) Validaci√≥n
    val_loss = evaluate(val_loader)
    
    # 4) Guardar checkpoints
    save_checkpoint(...)
```

---

### **Paso 5: Evaluaci√≥n**

```python
evaluate_on_test(G, dataset_root="datasets/catflw/test")
```

- Calcula L1, PSNR, SSIM en conjunto de test
- No afecta el entrenamiento (solo diagnosis)

---

### **Paso 6: Visualizaci√≥n**

```python
show_samples(G, test_loader, device, n=5)
```

- Muestra comparaci√≥n lado a lado:
  ```
  Input A (Landmarks) | Real B (Target) | Generado
  ```

---

## üí° Conceptos Avanzados

### 1. **¬øPor qu√© U-Net?**

Otras opciones:
- **Encoder-Decoder simple**: Pierde detalles espaciales en el bottleneck
- **ResNet**: No dise√±ado para imagen a imagen
- **U-Net**: Skip connections preservan informaci√≥n de cada escala

---

### 2. **¬øPor qu√© PatchGAN?**

Alternativas:
- **Discriminador global**: Clasifica imagen completa ‚Üí no captura texturas locales
- **PatchGAN**: Cada parche se eval√∫a independientemente ‚Üí mejor detalle

---

### 3. **Orden de Entrenamiento: D luego G**

```python
# Primero D
opt_D.zero_grad()
loss_D.backward()
opt_D.step()

# Luego G
opt_G.zero_grad()
loss_G.backward()
opt_G.step()
```

**Raz√≥n**: D debe estar actualizado para dar feedback correcto a G.

---

### 4. **`.detach()` en fake_B**

```python
fake_B = G(A).detach()  # Para entrenar D
```

**¬øPor qu√©?**
- Cuando entrenamos D, NO queremos actualizar G
- `.detach()` rompe el grafo computacional hacia G
- Sin esto, `loss_D.backward()` actualizar√≠a tambi√©n G

---

### 5. **BatchNorm vs sin Bias**

```python
nn.Conv2d(..., bias=False)
nn.BatchNorm2d(...)
```

**Raz√≥n**: BatchNorm normaliza y a√±ade par√°metros aprendibles (Œ≥, Œ≤), haciendo el bias redundante.

---

## üìà Hiperpar√°metros Clave

| Par√°metro | Valor | Justificaci√≥n |
|-----------|-------|---------------|
| `batch_size` | 32 | Balance memoria/velocidad |
| `lr` | 2e-4 | Est√°ndar GANs (Adam) |
| `beta1` | 0.5 | Momentum bajo para GANs |
| `lambda_L1` | 100 | Fuerte reconstrucci√≥n |
| `lambda_perc` | 10 | Moderado perceptual |
| `sigma` | 3-4 | Landmarks definidos |
| `epochs` | 100-200 | Convergencia t√≠pica |

---

## üõ†Ô∏è Troubleshooting

### **Problema**: Mode Collapse
- **S√≠ntoma**: G genera siempre la misma imagen
- **Soluci√≥n**: 
  - Reducir lr de G
  - Aumentar label smoothing
  - Agregar m√°s augmentation

---

### **Problema**: D demasiado fuerte
- **S√≠ntoma**: loss_D ‚Üí 0, loss_G no baja
- **Soluci√≥n**: 
  - Entrenar D cada 2-3 iteraciones
  - Reducir lr de D

---

### **Problema**: Im√°genes borrosas
- **S√≠ntoma**: Output realista pero desenfocado
- **Soluci√≥n**: 
  - Aumentar `lambda_L1`
  - Reducir `lambda_perc`
  - Verificar que usas L1 (no L2)

---

## üéØ Mejoras Posibles

1. **Spectral Normalization**: Estabiliza D
2. **Self-Attention**: Captura dependencias globales
3. **Progressive Growing**: Entrenar desde 64√ó64 ‚Üí 256√ó256
4. **Gradient Penalty**: Alternativa a label smoothing
5. **Multi-Scale Discriminator**: Eval√∫a m√∫ltiples resoluciones

---

## üìö Referencias

- [Pix2Pix Paper (2017)](https://arxiv.org/abs/1611.07004)
- [U-Net Architecture](https://arxiv.org/abs/1505.04597)
- [PatchGAN](https://arxiv.org/abs/1611.07004)
- [Perceptual Losses](https://arxiv.org/abs/1603.08155)

---

## ‚úÖ Checklist de Entrenamiento

- [ ] Dataset preparado correctamente (splits 80/10/10)
- [ ] Visualizar ejemplos del dataset (A y B alineados)
- [ ] Verificar GPU disponible (`torch.cuda.is_available()`)
- [ ] Iniciar con pocas √©pocas (10) para validar
- [ ] Monitorear losses (D y G deben oscilar, no diverger)
- [ ] Guardar checkpoints peri√≥dicamente
- [ ] Evaluar en test set al final
- [ ] Visualizar resultados cualitativos

---

## üöÄ Comando de Ejecuci√≥n

```python
# En Jupyter/VS Code notebook:
# Ejecutar todas las celdas en orden

# Si quieres solo entrenar:
G = train_pix2pix(
    dataset_root="datasets/catflw",
    epochs=100,
    batch_size=32,
    lambda_L1=100,
    lambda_perc=5
)

# Evaluar:
evaluate_on_test(G)
show_samples(G, test_loader, device, n=5)
```

---

**¬°Listo!** Ahora tienes una comprensi√≥n completa de c√≥mo funciona Pix2Pix y cada parte del c√≥digo. üé®‚ú®
